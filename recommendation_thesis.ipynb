{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections \n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse.linalg import svds\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from scipy import sparse\n",
    "from typing import List\n",
    "import scipy.sparse as sp\n",
    "\n",
    "for dirname, _, filenames in os.walk('/home/ebcffhh/Documents/personal/Masters/Thesis'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"/home/ebcffhh/Documents/personal/Masters/Thesis/ratings_Beauty.csv\", names = [\"userId\", \"ProductId\", \"Ratings\", \"Timestamp\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts=data.userId.value_counts()\n",
    "dataset_final=data[data.userId.isin(counts[counts>=25].index)]\n",
    "print('Number of users who have rated 25 or more items =', len(dataset_final))\n",
    "print('Number of unique users in the final data = ', dataset_final['userId'].nunique())\n",
    "print('Number of unique products in the final data = ', dataset_final['ProductId'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rated_products = data.groupby(by='userId',as_index=False)['Ratings'].count()\n",
    "print(rated_products)\n",
    "rated_products = rated_products[rated_products['Ratings'] < 20]\n",
    "new_dataset = data.loc[~((data.userId.isin(rated_products['userId']))),:]\n",
    "no_of_rated_products_per_user = new_dataset.groupby(by='userId')['Ratings'].count().sort_values(ascending=False)\n",
    "print(no_of_rated_products_per_user)\n",
    "print(new_dataset.ProductId.nunique())\n",
    "print(new_dataset.userId.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = new_dataset.userId.unique().shape[0]\n",
    "n_products = new_dataset.ProductId.unique().shape[0]\n",
    "product_list = new_dataset.ProductId.unique().tolist()\n",
    "print(n_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def GetTopN(predictions, n=10, minimumRating=4.0):\n",
    "      topN = defaultdict(list)\n",
    "\n",
    "\n",
    "      for userID, productId, actualRating, estimatedRating, _ in predictions:\n",
    "          if (estimatedRating >= minimumRating):\n",
    "              topN[userID].append((productId, estimatedRating))\n",
    "      for userID, ratings in topN.items():\n",
    "          ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "          topN[userID] = ratings[:n]\n",
    "\n",
    "      return topN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_customer_satisfaction(pred_u,k):\n",
    "  edt = {}\n",
    "  rating_list = defaultdict(list)\n",
    "  pred = pred_u.copy().groupby(['userId'])\n",
    "  for userId in pred.groups.keys():\n",
    "    sorted_pred_group = pred.get_group(userId).sort_values(['prediction'], ascending = False)\n",
    "    top_k = sorted_pred_group[:k]\n",
    "    top_k_g = top_k.groupby(by='userId')\n",
    "\n",
    "    for userId in top_k_g.groups.keys():\n",
    "      top_k_user_list = top_k_g.get_group(userId)\n",
    "      for _, groups in top_k_user_list.iterrows():\n",
    "        diff_ratings = groups['prediction'] - groups['actual']\n",
    "        rating_list.setdefault(groups['userId'], []).append(diff_ratings)\n",
    "      edt[userId] = (np.sum(rating_list.get(userId)))\n",
    "  return edt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_coverage(predicted: List[list], catalog: list) -> float:\n",
    "  predicted_flattened = [p for sublist in predicted for p in sublist]\n",
    "  unique_predictions = len(set(predicted_flattened))\n",
    "  prediction_coverage = round(unique_predictions/(len(catalog)* 1.0)*100,2)\n",
    "  return prediction_coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommender_precision(predicted: List[list], actual: List[list]) -> int:\n",
    "  def calc_precision(predicted, actual):\n",
    "      prec = [value for value in predicted if value in actual]\n",
    "      prec = np.round(float(len(prec)) / float(len(predicted)), 4)\n",
    "      return prec\n",
    "\n",
    "  precision_list = list(map(calc_precision, predicted, actual))\n",
    "  precision = np.mean(precision_list)\n",
    "  return precision, precision_list\n",
    "\n",
    "\n",
    "def recommender_recall(predicted: List[list], actual: List[list]) -> int:\n",
    "  def calc_recall(predicted, actual):\n",
    "      reca = [value for value in predicted if value in actual]\n",
    "      reca = np.round(float(len(reca)) / float(len(actual)), 4)\n",
    "      return reca\n",
    "\n",
    "  recall_list = list(map(calc_recall, predicted, actual))\n",
    "  recall = np.mean(recall_list)\n",
    "  return recall, recall_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def personalization(predicted: List[list]) -> float:\n",
    "    \"\"\"\n",
    "    Personalization measures recommendation similarity across users.\n",
    "    A high score indicates good personalization (user's lists of recommendations are different).\n",
    "    A low score indicates poor personalization (user's lists of recommendations are very similar).\n",
    "    A model is \"personalizing\" well if the set of recommendations for each user is different.\n",
    "    Parameters:\n",
    "    ----------\n",
    "    predicted : a list of lists\n",
    "        Ordered predictions\n",
    "        example: [['X', 'Y', 'Z'], ['X', 'Y', 'Z']]\n",
    "    Returns:\n",
    "    -------\n",
    "        The personalization score for all recommendations.\n",
    "    \"\"\"\n",
    "\n",
    "    def make_rec_matrix(predicted: List[list]) -> sp.csr_matrix:\n",
    "        df = pd.DataFrame(data=predicted).reset_index().melt(\n",
    "            id_vars='index', value_name='item',\n",
    "        )\n",
    "        df = df[['index', 'item']].pivot(index='index', columns='item', values='item')\n",
    "        df = pd.notna(df)*1\n",
    "        rec_matrix = sp.csr_matrix(df.values)\n",
    "        return rec_matrix\n",
    "\n",
    "    #create matrix for recommendations\n",
    "    predicted = np.array(predicted)\n",
    "    rec_matrix_sparse = make_rec_matrix(predicted)\n",
    "\n",
    "    #calculate similarity for every user's recommendation list\n",
    "    similarity = cosine_similarity(X=rec_matrix_sparse, dense_output=False)\n",
    "  \n",
    "\n",
    "    avg_sim = similarity.mean(axis=1)\n",
    "\n",
    "    #get indicies for upper right triangle w/o diagonal\n",
    "    upper_right = np.triu_indices(similarity.shape[0], k=1)\n",
    "\n",
    "    #calculate average similarity score of all recommended items in list\n",
    "    ils_single_user = np.mean(similarity[upper_right])\n",
    "    return avg_sim, (1 - ils_single_user)\n",
    "    #return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "reader = Reader()\n",
    "rating_data = Dataset.load_from_df(new_dataset[['userId', 'ProductId', 'Ratings']], reader)\n",
    "trainset, testset = train_test_split(rating_data, test_size=0.2,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import KNNWithMeans\n",
    "\n",
    "from surprise import accuracy\n",
    "\n",
    "k = [5, 10, 15, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "\n",
    "mae_svd = list()\n",
    "for i in k:\n",
    "  algo = SVD(n_factors=i, n_epochs=200)\n",
    "  algo.fit(trainset)\n",
    "  test_pred = algo.test(testset)\n",
    "  mae_svd.append(accuracy.mae(test_pred))\n",
    "  print(\"Mean Absolute Error for value k {} is \".format(i), accuracy.mae(test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "\n",
    "k = [5, 10, 15, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "reader = Reader()\n",
    "rating_data = Dataset.load_from_df(new_dataset[['userId', 'ProductId', 'Ratings']], reader)\n",
    "trainset, testset = train_test_split(rating_data, test_size=0.2,random_state=100)\n",
    "\n",
    "\n",
    "mae_knn = list()\n",
    "for i in k:\n",
    "  algo = KNNWithMeans(k=i, sim_options={'name':'pearson','user_based': True})\n",
    "  algo.fit(trainset)\n",
    "  test_pred = algo.test(testset)\n",
    "  mae_knn.append(accuracy.mae(test_pred))\n",
    "  print(\"Mean Absolute Error for value k {} is \".format(i), accuracy.mae(test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1_score(predictions, k):\n",
    "    threshold = 4\n",
    "    # First map the predictions to each user.\n",
    "    user_est_rating = defaultdict(list)\n",
    "    \n",
    "    for index, row in predictions.iterrows():\n",
    "        user_est_rating[row['userId']].append((row['prediction'], row['actual']))\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    f1_score = dict()\n",
    "    for uid, user_ratings in user_est_rating.items():\n",
    "        user_ratings.sort(key=lambda x:x[0], reverse=True)\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((r_ui >= threshold) for (_, r_ui) in user_ratings)\n",
    "        if math.isnan(n_rel):\n",
    "          print(\"nan value for rel\") \n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "        if math.isnan(n_rec_k):\n",
    "          print(\"nan value for rel\") \n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
    "                              for (est, true_r) in user_ratings[:k])\n",
    "        if math.isnan(n_rel_and_rec_k):\n",
    "          print(\"nan value for rel and rec\") \n",
    "        precision = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "        recall = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "        f1_score[uid] = 2 * ((precision * recall)/(precision+recall)) if (precision + recall) != 0 else 0\n",
    "    return f1_score\n",
    "\n",
    "def get_cus(predictions, k):\n",
    "    threshold = 4\n",
    "    # First map the predictions to each user.\n",
    "    user_est_rating = defaultdict(list)\n",
    "    for index, row in predictions.iterrows():\n",
    "      user_est_rating[row['userId']].append((row['prediction'], row['actual']))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    cus = defaultdict(list)\n",
    "    for uid, user_ratings in user_est_rating.items():\n",
    "        user_ratings.sort(key=lambda x:x[0], reverse=True)\n",
    "        for est, r_ui in user_ratings[:k]:\n",
    "          diff = r_ui - est\n",
    "          cus[uid].append(diff)\n",
    "    customerSatisfaction = {}\n",
    "    for key in cus:\n",
    "      customerSatisfaction[key] = np.sum(cus.get(key))/k\n",
    "    return customerSatisfaction\n",
    "\n",
    "def get_f1_score_nn(predictions, k):\n",
    "    threshold = 4\n",
    "    # First map the predictions to each user.\n",
    "    user_est_rating = defaultdict(list)\n",
    "    #for uid, iid, r_ui, est in predictions:\n",
    "    #    user_est_rating[uid].append((est, r_ui))\n",
    "    for index, row in predictions.iterrows():\n",
    "        user_est_rating[row['userId']].append((row['prediction'], row['actual']))\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    f1_score = dict()\n",
    "    for uid, user_ratings in user_est_rating.items():\n",
    "        user_ratings.sort(key=lambda x:x[0], reverse=True)\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((r_ui >= threshold) for (_, r_ui) in user_ratings)\n",
    "        if math.isnan(n_rel):\n",
    "          print(\"nan value for rel\") \n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "        if math.isnan(n_rec_k):\n",
    "          print(\"nan value for rel\") \n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
    "                              for (est, true_r) in user_ratings[:k])\n",
    "        if math.isnan(n_rel_and_rec_k):\n",
    "          print(\"nan value for rel and rec\") \n",
    "        precision = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "        recall = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "        f1_score[uid] = 2 * ((precision * recall)/(precision+recall)) if (precision + recall) != 0 else 0\n",
    "    return f1_score\n",
    "\n",
    "def cal_f1(test_pred, k):\n",
    "  f1_scores = get_f1_score(test_pred, k)\n",
    "  average_f1_score = sum(score for score in f1_scores.values())/ len(f1_scores)\n",
    "  return f1_scores , average_f1_score \n",
    "\n",
    "def get_accuracy(predictions,k):\n",
    "  user_est_rating = defaultdict(list)\n",
    "  for index, row in predictions.iterrows():\n",
    "      user_est_rating[row['userId']].append((row['prediction'], row['actual']))\n",
    "  accuracy_scores = dict()\n",
    "  for uid, user_ratings in user_est_rating.items():\n",
    "    scores = list()\n",
    "    user_ratings.sort(key=lambda x:x[0], reverse=True)\n",
    "    for (est, actual) in user_ratings[:k]:\n",
    "      diff = abs(actual - est)\n",
    "      scores.append(diff)\n",
    "    accuracy_scores[uid] = sum(score for score in scores)/len(scores)\n",
    "  return accuracy_scores\n",
    "\n",
    "def cal_accuracy(test_pred, k):\n",
    "  accuracy = get_accuracy(test_pred, k)\n",
    "  average_accuracy = sum(score for score in accuracy.values())/len(accuracy)\n",
    "  return accuracy , average_accuracy\n",
    "\n",
    "def get_shannon_entropy(predictions, product_list, no_of_recommendations):\n",
    "  recommendation_items = [item for sublist in predictions for item in sublist]\n",
    "  products = set(recommendation_items)\n",
    "  count_recommendation_items = collections.Counter(recommendation_items)\n",
    "  print(count_recommendation_items)\n",
    "  pi = list()\n",
    "  for product in products:\n",
    "    #if product in count_recommendation_items.keys():\n",
    "    pi.append(count_recommendation_items.get(product)/len(set(product_list)))\n",
    "  #e = -np.sum(pi*np.log(pi)/np.log(no_of_recommendations))\n",
    "  e = -np.sum(pi*np.log(pi))\n",
    "  return e\n",
    "  #print(\"Average diversity using shannon entropy for {} no of recommendations is {} \\n\".format(no_of_recommendations, -np.sum(pi*np.log(pi)/np.log(no_of_recommendations))))\n",
    "def get_shannon_entropy_new(predictions, product_list, no_of_recommendations):\n",
    "  recommendation_items = [item for sublist in predictions for item in sublist]\n",
    "  products = set(recommendation_items)\n",
    "  count_recommendation_items = collections.Counter(recommendation_items)\n",
    "  n_rec = sum(count_recommendation_items.values())\n",
    "  c = np.fromiter(count_recommendation_items.values(), dtype=int)\n",
    "  pi = c/n_rec\n",
    "  shannon_entropy = -np.sum(pi * np.log2(pi))\n",
    "  return shannon_entropy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def sorted_nicely( l ):\n",
    "    \"\"\" Sorts the given iterable in the way that is expected.\n",
    " \n",
    "    Required arguments:\n",
    "    l -- The iterable to be sorted.\n",
    " \n",
    "    \"\"\"\n",
    "    convert = lambda text: int(text) if text.isdigit() else text\n",
    "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n",
    "    return sorted(l, key = alphanum_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "import recmetrics\n",
    "import csv\n",
    "\n",
    "ratings_dataset = Dataset.load_from_df(new_dataset[['userId', 'ProductId', 'Ratings']],reader)\n",
    "\n",
    "trainset, testset = train_test_split(ratings_dataset, test_size=.2)\n",
    "\n",
    "product_list = set()\n",
    "\n",
    "for inner_pid in trainset.ir.keys():\n",
    "  product_list.add(trainset.to_raw_iid(inner_pid))\n",
    "\n",
    "algo = SVD(n_factors= 80, n_epochs=200)\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "test = pd.DataFrame(predictions)\n",
    "test = test.rename(columns={'uid':'userId', 'iid': 'productId', \n",
    "                            'r_ui':'actual', 'est':'prediction'})\n",
    "pred_user = test.copy().groupby('userId', as_index=False)['productId'].agg({'ratings': (lambda x: list(set(x)))})\n",
    "pred_user = pred_user.set_index(\"userId\")                            \n",
    "cf_model = test.pivot_table(index='userId', \n",
    "                            columns='productId', values='prediction').fillna(0)\n",
    "\n",
    "def get_users_predictions(user_id, n, model):\n",
    "    recommended_items = pd.DataFrame(model.loc[user_id])\n",
    "    recommended_items.columns = [\"predicted_rating\"]\n",
    "    recommended_items = recommended_items.sort_values('predicted_rating', ascending=False)    \n",
    "    recommended_items = recommended_items.head(n)\n",
    "    return recommended_items.index.tolist()\n",
    "\n",
    "def get_recs(model, k):\n",
    "    recs = []\n",
    "    for user in model.index:\n",
    "        cf_predictions = get_users_predictions(user, k, model)\n",
    "        recs.append(cf_predictions)\n",
    "    return recs\n",
    "\n",
    "productId_counts = dict(new_dataset.ProductId.value_counts())\n",
    "userId_counts = test['userId'].value_counts()\n",
    "\n",
    "diversity_svd = []\n",
    "novelty_svd = []\n",
    "coverage_svd = []\n",
    "f1_score_svd = []\n",
    "accuracy_svd = []\n",
    "\n",
    "# Top-n recommendations for each user\n",
    "no_of_recommendations = [5, 10, 15, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "#no_of_recommendations = [5]\n",
    "for k in no_of_recommendations:\n",
    "  recs = get_recs(cf_model, k)\n",
    "  pred_user[f'Top-{k} Recommendation'] = recs\n",
    "\n",
    "  # To calculate the f1_score\n",
    "  f1_scores_list_svd, average_f1_score_svd = cal_f1(test.copy() ,k)\n",
    "  print(\"The f1 score for {} recommendation is {} \\n\".format(k, average_f1_score_svd))\n",
    "  f1_score_svd.append(average_f1_score_svd)\n",
    "\n",
    "  # To calculate accuracy\n",
    "  accuracy_scores_svd, average_accuracy_svd = cal_accuracy(test.copy() ,k)\n",
    "  print(\"The accuracy score for {} recommendation is {} \\n\".format(k, average_accuracy_svd))\n",
    "  accuracy_svd.append(average_accuracy_svd)\n",
    "\n",
    "  # To calculate the diversity\n",
    "  diversity_scores_svd, average_diversity_svd = personalization(list(recs))\n",
    "  #diversity = get_shannon_entropy_new(recs, list(product_list), k)\n",
    "  print(\"The diversity score for {} recommendation is {} \\n\".format(k, average_diversity_svd))\n",
    "  diversity_svd.append(average_diversity_svd)\n",
    "\n",
    "  # To calculate the novelty\n",
    "  cf_novelty_svd, novelty_list_svd = recmetrics.novelty(recs, productId_counts, len(userId_counts), k)\n",
    "  print(\"The novelty score for {} recommendation is {} \\n\".format(k, cf_novelty_svd))\n",
    "  novelty_svd.append(cf_novelty_svd)\n",
    "\n",
    "  # To calculate the coverage\n",
    "  cf_coverage = recmetrics.catalog_coverage(list(recs), product_list, 100)\n",
    "  print(\"The coverage score for {} recommendation is {} \\n\".format(k, cf_coverage))\n",
    "  coverage_svd.append(cf_coverage)\n",
    "\n",
    "  # To calculate the customer satisfaction\n",
    "  edt_svd = get_customer_satisfaction(test, k)\n",
    "  print(\"The cusotmer satisfaction for {} recommendation is {}\".format(k,  np.mean(list(edt_svd.values()))))\n",
    "\n",
    "  filename = \"/home/ebcffhh/thesis/sorted_svd/metrics_svd_%s_recommendations.csv\" % k\n",
    "  with open(filename, 'w') as csv_file:  \n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(['userId', 'accuracy', 'f1_score', 'diversity', 'novelty', 'customer_satisfaction'])\n",
    "    for i, nov, (uid, acc_score), (_, f1_score) in zip(diversity_scores_svd, novelty_list_svd, accuracy_scores_svd.items(), f1_scores_list_svd.items()):\n",
    "       if uid in sorted_nicely(edt_svd.keys()):\n",
    "         writer.writerow([uid, acc_score, f1_score, (1 - i[0]), nov, edt_svd.get(uid)])\n",
    "\n",
    "print(accuracy_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNWithMeans\n",
    "from surprise import Dataset\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "import recmetrics\n",
    "\n",
    "#ratings_dataset = Dataset.load_from_df(new_dataset[['userId', 'ProductId', 'Ratings']],reader)\n",
    "\n",
    "#trainset, testset = train_test_split(ratings_dataset, test_size=.2)\n",
    "\n",
    "train_product_list_count = len(trainset.ir.keys())\n",
    "productId_counts = dict(new_dataset.ProductId.value_counts())\n",
    "userId_counts = new_dataset['userId'].value_counts()\n",
    "product_list = set()\n",
    "\n",
    "for inner_pid in trainset.ir.keys():\n",
    "  product_list.add(trainset.to_raw_iid(inner_pid))\n",
    "\n",
    "algo = KNNWithMeans(k=100, sim_options={'name':'pearson','user_based': True})\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "test = pd.DataFrame(predictions)\n",
    "test = test.rename(columns={'uid':'userId', 'iid': 'productId', \n",
    "                            'r_ui':'actual', 'est':'prediction'})\n",
    "pred_user = test.copy().groupby('userId', as_index=False)['productId'].agg({'ratings': (lambda x: list(set(x)))})\n",
    "pred_user = pred_user.set_index(\"userId\")    \n",
    "cf_model = test.pivot_table(index='userId', \n",
    "                            columns='productId', values='prediction').fillna(0)\n",
    "\n",
    "def get_users_predictions(user_id, n, model):\n",
    "    recommended_items = pd.DataFrame(model.loc[user_id])\n",
    "    recommended_items.columns = [\"predicted_rating\"]\n",
    "    recommended_items = recommended_items.sort_values('predicted_rating', ascending=False)    \n",
    "    recommended_items = recommended_items.head(n)\n",
    "    return recommended_items.index.tolist()\n",
    "\n",
    "def get_recs(model, k):\n",
    "    recs = []\n",
    "    for user in model.index:\n",
    "        cf_predictions = get_users_predictions(user, k, model)\n",
    "        recs.append(cf_predictions)\n",
    "    return recs    \n",
    "diversity_knn = []\n",
    "novelty_knn = []\n",
    "coverage_knn = []\n",
    "f1_score_knn = []\n",
    "accuracy_knn = []\n",
    "\n",
    "# Top-n recommendations for each user\n",
    "no_of_recommendations = [5, 10, 15, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "for k in no_of_recommendations:\n",
    "  \n",
    "  recs = get_recs(cf_model, k)\n",
    "  pred_user[f'Top-{k} Recommendation'] = recs\n",
    "\n",
    "  # To calculate the f1_score\n",
    "  f1_scores_knn, average_f1_score = cal_f1(test ,k)\n",
    "  print(\"The f1 score for {} recommendation is {} \\n\".format(k, average_f1_score))\n",
    "  f1_score_knn.append(average_f1_score)\n",
    "\n",
    "  # To calculate accuracy\n",
    "  accuracy_scores_knn, average_accuracy_knn = cal_accuracy(test.copy() ,k)\n",
    "  print(\"The accuracy score for {} recommendation is {} \\n\".format(k, average_accuracy_knn))\n",
    "  accuracy_knn.append(average_accuracy_knn)\n",
    "\n",
    "  # To calculate the diversity\n",
    "  diversity_scores_knn, average_diversity_knn = personalization(list(recs))\n",
    "  #diversity = get_shannon_entropy(recs, list(product_list), k)\n",
    "  print(\"The diversity score for {} recommendation is {} \\n\".format(k, average_diversity_knn))\n",
    "  diversity_knn.append(average_diversity_knn)\n",
    "\n",
    "  # To calculate the novelty\n",
    "  cf_novelty_knn, novelty_list_knn = recmetrics.novelty(list(recs), productId_counts, len(userId_counts), k)\n",
    "  print(\"The novelty score for {} recommendation is {} \\n\".format(k, cf_novelty_knn))\n",
    "  novelty_knn.append(cf_novelty_knn)\n",
    "\n",
    "  # To calculate the coverage\n",
    "  cf_coverage_knn = recmetrics.catalog_coverage(list(recs), product_list, 100)\n",
    "  print(\"The coverage score for {} recommendation is {} \\n\".format(k, cf_coverage_knn))\n",
    "  coverage_knn.append(cf_coverage_knn)\n",
    "  \n",
    "  # To calculate the customer satisfaction\n",
    "  edt_knn = get_customer_satisfaction(test, k)\n",
    "  print(\"The cusotmer satisfaction for {} recommendation is {}\".format(k,  np.mean(list(edt_knn.values()))))\n",
    "\n",
    "  filename = \"/home/ebcffhh/thesis/sorted_knn/metrics_knn_%s_recommendations.csv\" % k\n",
    "  with open(filename, 'w') as csv_file:  \n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(['userId', 'accuracy', 'f1_score', 'diversity', 'novelty', 'customer_satisfaction'])\n",
    "    for i, nov, (uid, acc_score), (_, f1_score) in zip(diversity_scores_knn, novelty_list_knn, accuracy_scores_knn.items(), f1_scores_knn.items()):\n",
    "       if uid in sorted_nicely(edt_knn.keys()):\n",
    "         writer.writerow([uid, acc_score, f1_score, (1 - i[0]), nov, edt_knn.get(uid)])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import one_hot\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(new_dataset, test_size = 0.2)\n",
    "\n",
    "user_encoder = LabelEncoder()\n",
    "product_encoder = LabelEncoder()\n",
    "\n",
    "train_user_ids = np.array([one_hot(d,10) for d in train_data['userId']])\n",
    "train_product_ids = np.array([one_hot(d,10) for d in train_data['ProductId']])\n",
    "test_product_ids = np.array([one_hot(d,10) for d in test_data['ProductId']])\n",
    "test_user_ids = np.array([one_hot(d,10) for d in test_data['userId']])\n",
    "\n",
    "num_users= train_user_ids.max()+1\n",
    "num_products = train_product_ids.max() + 1\n",
    "print(num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.normalization.batch_normalization import BatchNormalization\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Embedding, Flatten, Dense, Concatenate, dot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Embedding, Flatten, Dense, Concatenate, dot, Multiply, Dropout\n",
    "from keras.preprocessing.text import one_hot,Tokenizer\n",
    "import keras.layers\n",
    "from keras.optimizers import adam_v2\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_ncf_model(no_of_factors):\n",
    "\n",
    "  product_input = Input(shape = [1], name = \"Product-Input\")\n",
    "  user_input = Input(shape = [1], name = \"User-Input\")\n",
    "\n",
    "\n",
    "\n",
    "  # Product embedding for GMF\n",
    "  gmf_product_embedding = Embedding(n_products, no_of_factors, name= \"GMF-Product-Embedding\", embeddings_initializer=\"he_normal\")(product_input)\n",
    "  \n",
    "\n",
    "  # User embedding for GMF\n",
    "  gmf_user_embedding = Embedding(n_users, no_of_factors, name = \"GMF-User-Embedding\", embeddings_initializer=\"he_normal\")(user_input)\n",
    "  \n",
    "  # GMF layers\n",
    "  gmf_product_vec = Flatten(name = \"GMF-Flatten-Products\")(gmf_product_embedding)\n",
    "  gmf_user_vec = Flatten(name = \"GMF-Flatten-Users\")(gmf_user_embedding)\n",
    "  gmf_output = Multiply()([gmf_user_vec, gmf_product_vec])\n",
    "\n",
    "  \n",
    "  # Product embedding for MLP\n",
    "  mlp_product_embedding = Embedding(n_products, no_of_factors, name= \"MLP-Product-Embedding\", embeddings_initializer=\"he_normal\")(product_input)\n",
    "  \n",
    "\n",
    "  # User embedding for MLP\n",
    "  mlp_user_embedding = Embedding(n_users, no_of_factors, name = \"MLP-User-Embedding\", embeddings_initializer=\"he_normal\")(user_input)\n",
    "\n",
    "  # MLP layers\n",
    "\n",
    "  mlp_product_vec = Flatten(name = \"MLP-Flatten-Products\")(mlp_product_embedding)\n",
    "  mlp_user_vec = Flatten(name = \"MLP-Flatten-Users\")(mlp_user_embedding)\n",
    "\n",
    "  #Concatenate features\n",
    "  conc = Concatenate()([mlp_product_vec, mlp_user_vec])\n",
    "\n",
    "  fc1 = Dropout(0.2)(conc)\n",
    "  fc2 = Dense(64, activation='relu')(fc1)\n",
    "  fc3 = BatchNormalization()(fc2)\n",
    "\n",
    "  fc4 = Dropout(0.2)(fc3)\n",
    "  fc5 = Dense(32, activation='relu')(fc4)\n",
    "  fc6 = BatchNormalization()(fc5)\n",
    "\n",
    "  fc7 = Dropout(0.2)(fc6)\n",
    "\n",
    "  fc8 = Dense(16, activation='relu')(fc7)\n",
    "  fc9 = BatchNormalization()(fc8)\n",
    "\n",
    "  fc10 = Dropout(0.2)(fc9)\n",
    "  fc11 = Dense(8, activation='relu')(fc10)\n",
    "  final_conc = Concatenate()([gmf_output, fc11])\n",
    "  output = Dense(1, activation='relu')(final_conc)\n",
    "\n",
    " \n",
    "  #Create model and compile it\n",
    "  opt = keras.optimizers.adam_v2.Adam(learning_rate=0.001)\n",
    "  model = Model([user_input, product_input], output)\n",
    "  model.compile(loss='mean_absolute_error', optimizer=opt, metrics=['accuracy'] )\n",
    "  #model = Model([user_input, product_input], output)\n",
    "  #model.compile('adam', 'mean_absolute_error')\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "\n",
    "model = get_ncf_model(10)\n",
    "SVG(model_to_dot( model,  show_shapes=True, show_layer_names=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "no_of_factors = [5, 10, 15, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "\n",
    "mae_ncf = list()\n",
    "for k in no_of_factors:\n",
    "  model = get_ncf_model(k)\n",
    "  model.fit([train_user_ids, train_product_ids], train_data['Ratings'], epochs=3)\n",
    "  prediction = model.predict([test_user_ids, test_product_ids])\n",
    "  mae_ncf.append(mean_absolute_error(test_data['Ratings'], prediction))\n",
    "  print(\"Mean Absolute Error for value k {} is \".format(k), mean_absolute_error(test_data['Ratings'], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_ncf_model(40)\n",
    "model.fit([train_user_ids, train_product_ids], train_data['Ratings'], epochs=3)\n",
    "prediction = model.predict([test_user_ids, test_product_ids])\n",
    "predicted_df = pd.DataFrame({'userId': test_data['userId'], 'productId': test_data['ProductId'], 'actual': test_data['Ratings']})\n",
    "predicted_df['prediction'] = prediction\n",
    "pred_user = predicted_df.copy().groupby('userId', as_index=False)['productId'].agg({'actual': (lambda x: list(set(x)))})\n",
    "pred_user = pred_user.set_index(\"userId\")      \n",
    "cf_model = predicted_df.pivot_table(index='userId', \n",
    "                            columns='productId', values='prediction').fillna(0)\n",
    "\n",
    "userId_counts = test_data['ProductId'].value_counts()\n",
    "product_list = train_data.ProductId.unique().tolist()\n",
    "productId_counts = dict(new_dataset.ProductId.value_counts())\n",
    "\n",
    "def get_users_predictions(user_id, n, model):\n",
    "    recommended_items = pd.DataFrame(model.loc[user_id])\n",
    "    recommended_items.columns = [\"prediction\"]\n",
    "    recommended_items = recommended_items.sort_values('prediction', ascending=False)    \n",
    "    recommended_items = recommended_items.head(n)\n",
    "    return recommended_items.index.tolist()\n",
    "\n",
    "def get_recs(model, k):\n",
    "    recs = []\n",
    "    for user in model.index:\n",
    "        cf_predictions = get_users_predictions(user, k, model)\n",
    "        recs.append(cf_predictions)\n",
    "    return recs    \n",
    "\n",
    "diversity_nn = []\n",
    "novelty_nn = []\n",
    "coverage_nn = []\n",
    "f1_score_nn = []\n",
    "accuracy_nn = []\n",
    "\n",
    "# Top-n recommendations for each user\n",
    "no_of_recommendations = [5, 10, 15, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "for k in no_of_recommendations:\n",
    "  recs = get_recs(cf_model, k)\n",
    "  preds = pd.DataFrame(index=cf_model.index)\n",
    "  preds[f'Top-{k} Recommendation'] = recs\n",
    "\n",
    "  # To calculate the f1_score\n",
    "  f1_scores, f1_score = cal_f1(predicted_df ,k)\n",
    "  print(\"The f1 score for {} recommendation is {}\".format(k, f1_score))\n",
    "  f1_score_nn.append(f1_score)\n",
    "\n",
    "  # To calculate accuracy\n",
    "  accuracy_scores, accuracy = cal_accuracy(predicted_df ,k)\n",
    "  print(\"The accuracy score for {} recommendation is {} \\n\".format(k, accuracy))\n",
    "  accuracy_nn.append(accuracy)\n",
    "\n",
    "  # To calculate the diversity\n",
    "  diversity_scores, diversity = personalization(list(recs))\n",
    "  #diversity = get_shannon_entropy(recs, list(product_list), k)\n",
    "  print(\"The diversity score for {} recommendation is {} \\n\".format(k, diversity))\n",
    "  diversity_nn.append(diversity)\n",
    "\n",
    "  # To calculate the novelty\n",
    "  cf_novelty, novelty_list = recmetrics.novelty(list(recs), productId_counts, len(userId_counts), k)\n",
    "  print(\"The novelty score for {} recommendation is {} \\n\".format(k, cf_novelty))\n",
    "  novelty_nn.append(cf_novelty)\n",
    "\n",
    "  # To calculate the coverage\n",
    "  cf_coverage = recmetrics.catalog_coverage(list(recs), product_list, 100)\n",
    "  print(\"The coverage score for {} recommendation is {} \\n\".format(k, cf_coverage))\n",
    "  coverage_nn.append(cf_coverage)\n",
    "  \n",
    "  # To calculate the customer satisfaction\n",
    "  edt = get_customer_satisfaction(predicted_df, k)\n",
    "  print(\"The cusotmer satisfaction for {} recommendation is {}\".format(k,  np.mean(list(edt.values()))))\n",
    "\n",
    "  filename = \"/home/ebcffhh/thesis/dnn/metrics_dnn_%s_recommendations.csv\" % k\n",
    "  with open(filename, 'w') as csv_file:  \n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(['userId', 'accuracy', 'f1_score', 'diversity', 'novelty', 'customer_satisfaction'])\n",
    "    for i, nov, (uid, acc_score), (_, f1_score) in zip(diversity_scores, novelty_list, accuracy_scores.items(), f1_scores.items()):\n",
    "       if uid in sorted(edt.keys()):\n",
    "         writer.writerow([uid, acc_score, f1_score, (1 - i[0]), nov, edt.get(uid)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [5, 10, 15, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "\n",
    "plt.xlabel('No of factors')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "ax=plt.gca()\n",
    "ax.locator_params('y', nbins=10)\n",
    "\n",
    "plt.locator_params('x', nbins=20)\n",
    "plt.plot(k, mae_knn, label = \"KNN\")\n",
    "plt.plot(k, mae_svd, label = \"SVD\")\n",
    "plt.plot(k, mae_ncf, label = \"DNN\")\n",
    "plt.scatter(k,mae_knn,s=50,color='red',zorder=2)\n",
    "plt.scatter(k,mae_svd,s=50,color='green',zorder=2)\n",
    "plt.scatter(k,mae_ncf,s=50,color='brown',zorder=2)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"mae\")\n",
    "print(\"mae_svd\",mae_svd)\n",
    "print(\"mae_knn\",mae_knn)\n",
    "print(\"mae_ncf\",mae_ncf)\n",
    "\n",
    "\n",
    "plt.xlabel('No of Recommendations')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "ax=plt.gca()\n",
    "ax.locator_params('y', nbins=10)\n",
    "\n",
    "plt.locator_params('x', nbins=20)\n",
    "plt.plot(k, accuracy_knn, label = \"KNN\")\n",
    "plt.plot(k, accuracy_svd, label = \"SVD\")\n",
    "plt.plot(k, accuracy_nn, label = \"DNN\")\n",
    "plt.scatter(k,accuracy_knn,s=50,color='red',zorder=2)\n",
    "plt.scatter(k,accuracy_svd,s=50,color='green',zorder=2)\n",
    "plt.scatter(k,accuracy_nn,s=50,color='brown',zorder=2)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"accuracy\")\n",
    "print(\"accuracy_svd\",accuracy_svd)\n",
    "print(\"accuracy_knn\",accuracy_knn)\n",
    "print(\"accuracy_dnn\",accuracy_nn)\n",
    "\n",
    "plt.xlabel('No of Recommendations')\n",
    "plt.ylabel('f1_Score')\n",
    "ax=plt.gca()\n",
    "ax.locator_params('y', nbins=15)\n",
    "plt.locator_params('x', nbins=20)\n",
    "plt.plot(k, f1_score_knn, label = \"KNN\")\n",
    "plt.plot(k, f1_score_svd, label = \"SVD\")\n",
    "plt.plot(k, f1_score_nn, label = \"DNN\")\n",
    "\n",
    "\n",
    "plt.scatter(k,f1_score_knn,s=50,color='red',zorder=2)\n",
    "plt.scatter(k,f1_score_svd,s=50,color='green',zorder=2)\n",
    "plt.scatter(k,f1_score_nn,s=50,color='brown',zorder=2)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"f1_Score\")\n",
    "print(\"f1_Score_nn\", f1_score_nn)\n",
    "print(\"f1_Score_knn\", f1_score_knn)\n",
    "print(\"f1_Score_svd\", f1_score_svd)\n",
    "\n",
    "plt.xlabel('No of Recommendations')\n",
    "plt.ylabel('Diversity')\n",
    "ax=plt.gca()\n",
    "ax.locator_params('y', nbins=25)\n",
    "plt.locator_params('x', nbins=20)\n",
    "for i, txt in enumerate(diversity_nn):\n",
    "    ax.annotate(round(txt, 5), (k[i],diversity_nn[i]),  fontsize=8)\n",
    "plt.plot(k, diversity_nn, label = \"DNN\")\n",
    "plt.scatter(k,diversity_nn,s=20,color='brown',zorder=1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.xlabel('No of Recommendations')\n",
    "plt.ylabel('Diversity')\n",
    "ax=plt.gca()\n",
    "ax.locator_params('y', nbins=25)\n",
    "plt.locator_params('x', nbins=20)\n",
    "for i, txt in enumerate(diversity_knn):\n",
    "    ax.annotate(round(txt, 5), (k[i],diversity_knn[i]),  fontsize=8)\n",
    "plt.plot(k, diversity_knn, label = \"KNN\")\n",
    "plt.scatter(k,diversity_knn,s=20,color='red',zorder=2)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.xlabel('No of Recommendations')\n",
    "plt.ylabel('Diversity')\n",
    "ax=plt.gca()\n",
    "ax.locator_params('y', nbins=25)\n",
    "plt.locator_params('x', nbins=20)\n",
    "for i, txt in enumerate(diversity_svd):\n",
    "    ax.annotate(round(txt, 5), (k[i],diversity_svd[i]),  fontsize=8)\n",
    "plt.plot(k, diversity_svd, label = \"SVD\")\n",
    "\n",
    "\n",
    "plt.scatter(k,diversity_svd,s=20,color='red',zorder=2)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"diversity\")\n",
    "print(\"divesity_knn\", diversity_knn)\n",
    "print(\"diversity_nn\",diversity_nn)\n",
    "print(\"diversity_svd\",diversity_svd)\n",
    "\n",
    "plt.xlabel('No of Recommendations')\n",
    "plt.ylabel('Novelty')\n",
    "ax=plt.gca()\n",
    "ax.locator_params('y', nbins=15)\n",
    "plt.locator_params('x', nbins=20)\n",
    "plt.plot(k, novelty_knn, label = \"KNN\")\n",
    "plt.plot(k, novelty_svd, label = \"SVD\")\n",
    "plt.plot(k, novelty_nn, label = \"DNN\")\n",
    "\n",
    "plt.scatter(k,novelty_knn,s=50,color='red',zorder=2)\n",
    "plt.scatter(k,novelty_svd,s=50,color='green',zorder=2)\n",
    "plt.scatter(k,novelty_nn,s=50,color='black',zorder=2)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"novelty\")\n",
    "print(\"novelty_knn\", novelty_knn)\n",
    "print(\"novelty_nn\",novelty_nn)\n",
    "print(\"novelty_svd\",novelty_svd)\n",
    "\n",
    "plt.xlabel('No of Recommendations')\n",
    "plt.ylabel('Coverage')\n",
    "ax=plt.gca()\n",
    "ax.locator_params('y', nbins=15)\n",
    "plt.locator_params('x', nbins=20)\n",
    "plt.plot(k, coverage_knn, label = \"KNN\")\n",
    "plt.plot(k, coverage_svd, label = \"SVD\")\n",
    "plt.plot(k, coverage_nn, label = \"DNN\")\n",
    "\n",
    "plt.scatter(k,coverage_knn,s=50,color='blue',zorder=2)\n",
    "plt.scatter(k,coverage_svd,s=50,color='grey',zorder=2)\n",
    "plt.scatter(k,coverage_nn,s=50,color='orange',zorder=2)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
